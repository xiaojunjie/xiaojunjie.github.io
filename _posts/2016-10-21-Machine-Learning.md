---
layout: post
title: 机器学习
categories: note
tags: coursera
---

[斯坦福大学公开课](//coursera.org/learn/machine-learning)

## 介绍

- 监督学习：把样本分成具体的类，如邮件分类。
- 无监督学习：把样本分成若干类，不知道各类特性，如百度新闻分类

## 线性回归
- 方案：h(x) = θ'X
- 代价函数：J(θ) = 1/2m * sum[ (h(x) - y)^2 ]
- 梯度下降： θ = θ - a/m*( θ‘X - Y )'X

## 正规方程
- θ = inv(X'X)X'Y 
- 求逆O(n^3),适用于n<10000
- 正规方程仅用了线性回归

## 逻辑回归  
- 方案：h(x) = 1/[1+exp(-θ'X)]
- 代价函数：  
  if y=1 	**-log(h(x))**    
  if y=0	 **-log(1-h(x))**
- 梯度下降： θ = θ - a/m*( θ‘X - Y )'X
- 分为多类时(y = 1 2 ... n)  
  y=1为1类，y=其他为另一类，共两类，即化为2类问题  
  最后各类均有θ，求max(h(x))的索引

## 过拟合  
- 方法  
	1.丢弃一些不能帮助我们正确预测的特征。  
	2.正则化(保留所有的特征,但是减少参数的大小)  
-  线性和逻辑  
  J = J + 入/2m*sum(θ) **θ不包含θ0**当不处罚θ0时  
  θ = (1-入/m)θ 
  **θ0 = θ0**    
- 正规方程  
  θ = inv(X'X-入ones(n+1,n+1))X'Y **ones的（0,0）为0**  

## 神经网络
- 概念  
  基于[逻辑回归](./#逻辑回归)，多类。各类均有1个向量θ，同理最后求其max(概率)。特别之处在于中间有递归多次。  
  比如1000样本分成10类，按逻辑回归思路，一步到位，而神经网络会把其先分为500,再把500分成100,最后10.  
  **每步的样本输入为h(x)**,而非Xθ，否则经过多次递归肯定出错。  
  而逻辑回归只有一步 && h(x)单调增，故max(Xθ)效果同max(h(x))  
  当然，每步和输入的样本维度要+1  
  ![](/assets/dist/img/2016-10-27 16-49-51屏幕截图.png)  
  蓝圈x，红圈h(x)